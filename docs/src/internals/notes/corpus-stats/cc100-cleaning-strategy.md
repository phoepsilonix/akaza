# CC-100 Japanese クリーニング戦略

## データ品質の調査結果

CC-100 Japanese (`ja.txt.xz`) は CCNet パイプラインで Common Crawl から抽出された日本語テキスト。

| 指標 | 値 |
|---|---|
| トーカナイズ済みサイズ | 169GB (jawiki の **7.7 倍**) |
| 文書数 | 6,560 万 (jawiki の **44 倍**) |
| 200 文字未満の文書 | **55%** |
| wfreq の純 ASCII エントリ | **25.9%** (`com`, `https`, `www` 等) |
| wfreq の純数字エントリ | **7%** |
| LaTeX 残骸 | `displaystyle` 17 万回等 |
| 重複文書 | ~0% (CCNet で除去済み) |
| 非日本語文書 | ~0.02% |
| ひらがな比率の中央値 | ~50% |

## フィルタリングルール

`scripts/extract-cc100.py` で文書単位のフィルタを適用する。

### 1. 最小文書長フィルタ (200 文字未満を除外)

- **影響**: 全文書の約 55% (~3,600 万文書) を除去
- **根拠**: 短い断片はナビゲーションテキスト、定型文、URL 断片等が多く、n-gram 統計にノイズを加えるだけ

### 2. ひらがな比率フィルタ (10% 未満を除外)

- **影響**: ~0.8% の文書を追加で除去
- **根拠**: 自然な日本語文にはひらがなが一定割合含まれる。ひらがなが極端に少ない文書は、コードブロック、数式、外国語テキスト等の可能性が高い

### 3. 行の繰り返しフィルタ (30% 以上重複行で除外)

- **影響**: 少量の文書を追加で除去
- **根拠**: コピペスパム、商品リストの繰り返し等を排除

### `--no-filter` オプション

すべてのフィルタを無効化し、生テキストをそのまま出力する。デバッグ・比較検証用。

## 配布物の分離

フィルタリング後も CC-100 は jawiki に比べて大幅にサイズが大きく、品質も異なるため、2 つのバリアントで配布する。

| バリアント | コーパス | ターゲット |
|---|---|---|
| `dist/` (デフォルト) | jawiki + 青空文庫 | `make dist` |
| `dist-full/` | jawiki + 青空文庫 + CC-100 | `make dist-full` |

`dist-full/` 内のファイル名は `dist/` と同一 (接尾辞なし) のため、利用側での差し替えが容易。

`make release` は両方の tarball (`akaza-corpus-stats.tar.gz`, `akaza-corpus-stats-full.tar.gz`) を GitHub Release にアップロードする。

## CC100_LIMIT

`CC100_LIMIT` は `extract-cc100.py` の `--limit` に渡される値で、抽出する文書数の上限。

- **デフォルト**: `5000000` (500 万文書)
- フィルタ通過率 ~45% を考慮すると、出力は ~225 万文書 (jawiki の ~150 万と同程度)
- `make CC100_LIMIT=0` で無制限 (全 6,560 万文書を処理)

## 不採用とした手法

| 手法 | 理由 |
|---|---|
| 重複除去 | CC-100 は CCNet パイプラインで既に重複除去済み (実測 ~0%) |
| FastText 言語判定 | 非日本語は 0.02% と極めて少なく、外部依存に見合わない |
| Perplexity フィルタ | 効果は期待できるが言語モデルの準備が必要で、現段階では見送り |

## 今後の改善候補

- **Perplexity フィルタ**: KenLM 等の軽量 LM で文の perplexity を計算し、異常値を除外
- **NG ワードリスト**: 不適切コンテンツの除外
- **URL/メールアドレス除去**: 行レベルでの前処理
- **vocab threshold の調整**: CC-100 込みの場合、`--threshold` の引き上げを検討
