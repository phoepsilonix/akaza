# 構造化パーセプトロン実装の評価レポート

## 背景

[learn-corpus 改善実験](learn-corpus-improvement.md)で、カウントベース更新（加算/減算）は evaluate スコアに影響を与えないことが判明した。原因はコスト関数 `cost = -log10((count + α) / (total + α + V))` の非線形性にあり、カウント ±2000 程度では高頻度語のコスト変化が微小（~0.009）で変換結果を変えるに至らなかった。

この問題を解決するため、[構造化パーセプトロン](structured-perceptron.md)の方針に基づき、**カウントではなくコストに直接加算するスコアベースの構造化パーセプトロン**を実装した。

## 実装概要

### 変更点

1. **OnMemory LM に `adjustment: HashMap<K, f32>` を追加**: `find()` / `get_edge_cost()` が返す値に adjustment を加算
2. **学習ロジックの書き換え**: 不正解時に以下の更新を実行
   - 教師パスの全特徴（unigram/bigram/skip-bigram）: コスト -= step_size（安くする）
   - 予測パスの全特徴: コスト += step_size（高くする）
3. **CLI 引数**: `--delta` (u32) → `--step-size` (f32, default 0.5)

### パラメータ

```
step_size = 0.5
may_epochs = 10, should_epochs = 100, must_epochs = 10000
```

## 実験結果

| 手法 | Good | Top-5 | Bad | 再現率 | 学習時間 |
|---|---|---|---|---|---|
| ベースライン（学習済みモデル） | 6719 | 416 | 3930 | 93.267% | — |
| 構造化パーセプトロン (step=0.5) | 6614 | 492 | 3959 | 92.768% | 18:18 |
| 差分 | **-105** | +76 | +29 | **-0.499%** | — |

**結果: 0.5%の精度低下。** Good が 105 文減少、Bad が 29 文増加。Top-5 が 76 増加したのは、以前 Top-1 で正解していた文が Top-5 に落ちたことを示す。

## 差分分析

### 概要

| カテゴリ | 件数 |
|---|---|
| 新版でBADになった（以前は正解） | 431 |
| 新版で修正された（以前はBAD） | 405 |
| 両方BADだが変換が変わった | ~820 |
| 純損失 | 26 |

### 新規リグレッション上位パターン

| パターン | 件数 | 説明 |
|---|---|---|
| 「き」→「季」 | 51件 | 文節区切り失敗で語末「き」が独立文節「季」に |
| 「を」→「ヲ」 | 33件 | 助詞がカタカナ化 |
| 「きのう」→「機能」 | 19件 | 「昨日」が「機能」に誤変換 |
| 「きゅう」: 急→旧 | 11件 | 同音異義語の優先度逆転 |
| 「ご」→「御」 | 9件 | ひらがなであるべき接頭辞の漢字化 |
| 「にほん」→「に本」 | 7件 | 文節区切りの破壊 |
| 「かく」: 書く→各 | 7件 | 同音異義語の優先度逆転 |

### 修正されたパターン（改善点）

| パターン | 件数 | 説明 |
|---|---|---|
| 「き」: 木→気 | 35件 | 旧版の「気→木」誤変換が修正 |
| 「さい」: 賽→再 | 14件 | 旧版の「再→賽」誤変換が修正 |
| 「あき」: 空き→秋 | 7件 | 旧版の「秋→空き」誤変換が修正 |
| 「えん」: 縁→円 | 6件 | 旧版の「円→縁」誤変換が修正 |

## 考察

### 1. 学習は確かに効果がある

前回のカウントベース実験では evaluate スコアが完全に同一（変化ゼロ）だったが、今回はスコアが明確に変動した。**コストに直接加算するアプローチは、モデルの変換結果を実際に変える力がある**ことが確認された。

### 2. しかし精度は低下した

改善（405文修正）と劣化（431文リグレッション）がほぼ拮抗しているが、劣化がやや上回り、全体として 0.5% の精度低下となった。

### 3. 劣化の原因分析

主要な原因は以下の通り:

**a) 学習コーパスの規模と評価コーパスのドメイン差**

学習コーパス（training-corpus/）は手作業で作成した少量のデータ（may: 数百行、should: 数百行、must: 10行程度）。この限られたデータで学んだ重みが、11,065 件の評価コーパスに対して汎化できていない。特に「季」「ヲ」「機能」などのリグレッションは、学習コーパスの偏りが評価コーパスのドメインと合っていないことを示唆。

**b) step_size = 0.5 の妥当性**

step_size 0.5 は、典型的なコスト値（2〜8 程度）に対してかなり大きい。数十〜数百エポック繰り返すと adjustment が数十に達し、元のコスト構造を大きく歪める可能性がある。

**c) 正則化の欠如**

現在の実装には正則化がない。過学習により、学習コーパスにフィットする代わりに汎化性能が低下している。

### 4. 今後の改善方向

1. **step_size の縮小**: 0.1 や 0.05 など、より保守的な値を試す
2. **averaged perceptron**: 全エポックの重みの平均を取ることで過学習を抑制
3. **学習コーパスの拡充**: 評価コーパスとドメインが重なるデータの追加
4. **early stopping**: 検証セットで精度が低下し始めたらエポックを打ち切る
5. **特徴の重複除外**: 教師パスと予測パスで共通する特徴をスキップ（相殺される分の無駄を省く）
6. **学習率スケジューリング**: エポックが進むにつれて step_size を減衰させる

## 結論

構造化パーセプトロンによるスコアベース学習は、カウントベース学習と異なり **モデルの変換結果を実際に変える力がある**ことが確認された。しかし、現在のパラメータ設定（step_size=0.5）と限られた学習コーパスでは精度が 0.5% 低下する結果となった。

正則化手法（averaged perceptron）、学習率の調整、学習コーパスの拡充が次のステップとなる。
